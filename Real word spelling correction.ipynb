{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-word Spell Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "import string\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edit Distance Approach (Add non-dp approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def editDistance(str1, str2, m, n): \n",
    "    # Create a table to store results of subproblems \n",
    "    dp = [[0 for x in range(n+1)] for x in range(m+1)] \n",
    "  \n",
    "    # Fill d[][] in bottom up manner \n",
    "    for i in range(m+1): \n",
    "        for j in range(n+1): \n",
    "  \n",
    "            # If first string is empty, only option is to \n",
    "            # insert all characters of second string \n",
    "            if i == 0: \n",
    "                dp[i][j] = j    # Min. operations = j \n",
    "  \n",
    "            # If second string is empty, only option is to \n",
    "            # remove all characters of second string \n",
    "            elif j == 0: \n",
    "                dp[i][j] = i    # Min. operations = i \n",
    "  \n",
    "            # If last characters are same, ignore last char \n",
    "            # and recur for remaining string \n",
    "            elif str1[i-1] == str2[j-1]: \n",
    "                dp[i][j] = dp[i-1][j-1] \n",
    "  \n",
    "            # If last character are different, consider all \n",
    "            # possibilities and find minimum \n",
    "            else: \n",
    "                dp[i][j] =     min(dp[i][j-1] + 1,        # Insert \n",
    "                                   dp[i-1][j] + 1,        # Remove \n",
    "                                   dp[i-1][j-1] + 1)    # Replace \n",
    "  \n",
    "    return dp[m][n] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edit Distance between \"form\" and \"from\" =  1\n"
     ]
    }
   ],
   "source": [
    "print('Edit Distance between \"form\" and \"from\" = ',editDistance('frtm','from',4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ind(char): # Returns the index of character in alphabets\n",
    "    return string.ascii_lowercase.index(char)\n",
    "\n",
    "def get_weights():\n",
    "    insert = delete = sub = trans = [[0 for i in range(26)] for j in range(26)]\n",
    "    for i in open('errors.txt').readlines():\n",
    "        exp,val = i.split('\\t')\n",
    "        val = [i for i in val.split() if i.isdigit()]\n",
    "        val = int(''.join(val))\n",
    "        l,r = exp.split('|')\n",
    "        chars = string.ascii_lowercase\n",
    "        if(len(l)==2 and len(r)==1 and l[0]==r and l[0] in chars and l[1] in chars):\n",
    "            delete[ind(l[0])][ind(l[1])] = val\n",
    "        elif(len(l)==1 and len(r)==2 and l==r[0] and r[0] in chars and r[1] in chars):\n",
    "            insert[ind(r[0])][ind(r[1])] = val\n",
    "        elif(len(l)==1 and len(r)==1 and l in chars and r in chars):\n",
    "            sub[ind(l)][ind(r)] = val\n",
    "        elif(len(l)==2 and len(r)==2 and l[0]==r[1] and l[1]==r[0] and l[0] in chars and l[1] in chars):\n",
    "            trans[ind(l[0])][ind(l[1])] = val\n",
    "    return [delete,insert,sub,trans]\n",
    "\n",
    "def weightedEditDistance(s1,s2,m,n,insert,delete,sub):\n",
    "    D = [[0 for x in range(n+1)] for y in range(m+1)]\n",
    "            \n",
    "    for i in range(m+1):\n",
    "        for j in range(n+1):\n",
    "            \n",
    "            if(i==0):\n",
    "                D[i][j] = j\n",
    "                \n",
    "            elif(j==0):\n",
    "                D[i][j] = i\n",
    "                \n",
    "            elif(s1[i-1] == s2[j-1]):\n",
    "                D[i][j] = D[i-1][j-1]\n",
    "                \n",
    "            else:\n",
    "                a = D[i-1][j] + delete[ind(s1[i-1])]\n",
    "                b = D[i][j-1] + insert[ind(s2[j-1])]\n",
    "                c = D[i-1][j-1] + sub[ind(s1[i-1])][ind(s2[j-1])]\n",
    "                \n",
    "                D[i][j] = min(a,b,c)\n",
    "    \n",
    "    return D[m][n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random errors for insertion, deletion and substitution. Can be taken from corpus with spelling mistakes and their corrections.\n",
    "insert = np.random.randint(1,10,size=26)\n",
    "delete = np.random.randint(1,10,size=26)\n",
    "sub = np.random.randint(1,10,size=(26,26))\n",
    "\n",
    "weightedEditDistance('from','frtm',4,4,insert,delete,sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a spell Corrector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading 'big.txt' - Concatenation of public domain book excerpts from Project Gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines =  128458\n",
      "Number of Tokens =  1105736\n"
     ]
    }
   ],
   "source": [
    "data = open('big.txt',encoding='utf-8').read()\n",
    "print(\"Number of lines = \",len(data.split('\\n')))\n",
    "data = data.replace('\\n',' ').replace('\\t',' ').replace('-',' ').replace('â€”',' ')\n",
    "data = data.replace('_','')\n",
    "data = re.sub(r'[^\\w\\s]','',data)\n",
    "data = data.lower().strip().split(' ')\n",
    "data = list(filter(None,data))\n",
    "print(\"Number of Tokens = \",len(data))\n",
    "\n",
    "WORDS = Counter(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding closest word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def P(word, N=sum(WORDS.values())): \n",
    "    # Returns probability of a word in vocabulary.\n",
    "    return WORDS[word] / N\n",
    "\n",
    "\n",
    "def word_candidates(word):\n",
    "    # Returns the words that are edit distance 0,1 or 2 away from the given word. Inspired from Peter Norvig\n",
    "    ed_0 = set()\n",
    "    ed_1 = set()\n",
    "    ed_2 = set()\n",
    "    for w in WORDS:\n",
    "        ed = editDistance(word,w,len(word),len(w))\n",
    "        if(ed > 2):\n",
    "            continue\n",
    "        elif(ed==0):\n",
    "            ed_0.add(w)\n",
    "        elif(ed==1):\n",
    "            ed_1.add(w)\n",
    "        elif(ed==2):\n",
    "            ed_2.add(w)\n",
    "    return [ed_0,ed_1,ed_2,{word}]\n",
    "\n",
    "def closest_word(word):\n",
    "    # Chooses closest word according to their frequency. Highest priority given to words with least edit distance.\n",
    "    for i in word_candidates(word):\n",
    "        if(i):\n",
    "            return max(i,key=P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def P(word, N=sum(WORDS.values())): \n",
    "    # Returns probability of a word in vocabulary.\n",
    "    return WORDS[word] / N\n",
    "\n",
    "\n",
    "def word_candidates(word):\n",
    "    # Returns the words that are edit distance 0,1 or 2 away from the given word. Inspired from Peter Norvig\n",
    "    ed_0 = set()\n",
    "    ed_1 = set()\n",
    "    ed_2 = set()\n",
    "    for w in WORDS:\n",
    "        ed = editDistance(word,w,len(word),len(w))\n",
    "        if(ed > 2):\n",
    "            continue\n",
    "        elif(ed==0):\n",
    "            ed_0.add(w)\n",
    "        elif(ed==1):\n",
    "            ed_1.add(w)\n",
    "        elif(ed==2):\n",
    "            ed_2.add(w)\n",
    "    return [ed_0,ed_1,ed_2,{word}]\n",
    "\n",
    "def closest_word(word):\n",
    "    # Chooses closest word according to their frequency. Highest priority given to words with least edit distance.\n",
    "    for i in word_candidates(word):\n",
    "        if(i):\n",
    "            return max(i,key=P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a sentence corrector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build without assumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumption: Only 1 mistake per sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bigrams = Counter(ngrams(data,2))\n",
    "def bigram_prob(sent):\n",
    "    out = P(sent[0])\n",
    "    for i in range(1,len(sent)):\n",
    "        out *= bigrams[(sent[i-1],sent[i])] / WORDS[sent[i-1]]\n",
    "    return out\n",
    "\n",
    "def sent_candidates(sent):\n",
    "    cands = []\n",
    "    sent = sent.split()\n",
    "    word_present = [s in WORDS for s in sent]\n",
    "    if all(word_present): \n",
    "        cands.append(sent)\n",
    "        for i in range(len(sent)):\n",
    "            words = word_candidates(sent[i])[1]\n",
    "            for j in words:\n",
    "                l = sent.copy()\n",
    "                l[i] = j\n",
    "                cands.append(l)\n",
    "    else: \n",
    "        idx = word_present.index(0)\n",
    "        words = word_candidates(sent[idx])[1]\n",
    "        for i in words:\n",
    "            l = sent.copy()\n",
    "            l[idx] = i\n",
    "            cands.append(l)\n",
    "    return cands\n",
    "\n",
    "def closest_sent(sent):\n",
    "    return ' '.join(max(sent_candidates(sent),key=bigram_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i have an apple'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest_sent('i have an apply')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 25-40 % of spelling errors are real words kukich 1992"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram context based spell check (with incorrect spelling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trigram and quadgram context based spell check (with incorrect spelling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram context based spell check (with correct spelling - word exists in dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use word embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
